{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy\n",
    "# randoml\n",
    "from random import shuffle\n",
    "# classifierfrom deepdist import DeepDist\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdist import DeepDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=yarn) created by __init__ at <ipython-input-3-33ce3f59c0b1>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-33ce3f59c0b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 296\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=yarn) created by __init__ at <ipython-input-3-33ce3f59c0b1>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "corpus =sc.textFile('hdfs:///user/hadoop/data/tagged_docs_shuffled.txt').map(\n",
    "    lambda s: TaggedDocument(s[19:].split(), [s[:18]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(model, taggedData):  # executes on workers\n",
    "#     print type(taggedData)\n",
    "#     sentence = []\n",
    "#     for i in taggedData:\n",
    "#         sentence.append(i)\n",
    "    syn0, syn1, doctag = model.wv.syn0.copy(), model.syn1neg.copy(), model.docvecs.vectors_docs.copy()\n",
    "    model.train(taggedData,total_examples=model.corpus_count,epochs=3)\n",
    "    return {'syn0': model.wv.syn0 - syn0,'syn1':model.syn1neg-syn1,'doctag':model.docvecs.vectors_docs-doctag}\n",
    "\n",
    "def descent(model, update): # executes on master\n",
    "    #print model.docvecs.doctag_syn0.shape\n",
    "    #print update['doctag'].shape\n",
    "    model.wv.syn0 += update['syn0']\n",
    "    model.syn1neg += update['syn1']\n",
    "    model.docvecs.vectors_docs += update['doctag']\n",
    "    #print update['taggedDoc'].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.23 s, sys: 1.15 s, total: 10.4 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collectedCorpus = corpus.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 17s, sys: 16.7 s, total: 8min 34s\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2VecModel = Doc2Vec(collectedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening to 0.0.0.0:5000...\n",
      "\n",
      "*** Master: 192.168.91.1:5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.91.3 - - [24/Apr/2018 11:50:52] \"GET /model HTTP/1.1\" 200 -\n",
      "192.168.91.2 - - [24/Apr/2018 11:50:52] \"GET /model HTTP/1.1\" 200 -\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n",
      "  \n",
      "192.168.91.2 - - [24/Apr/2018 11:51:32] \"POST /update HTTP/1.1\" 200 -\n",
      "192.168.91.3 - - [24/Apr/2018 11:51:33] \"POST /update HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Master: 192.168.91.1:5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.91.3 - - [24/Apr/2018 11:51:36] \"GET /model HTTP/1.1\" 200 -\n",
      "192.168.91.2 - - [24/Apr/2018 11:51:36] \"GET /model HTTP/1.1\" 200 -\n",
      "192.168.91.3 - - [24/Apr/2018 11:52:16] \"POST /update HTTP/1.1\" 200 -\n",
      "192.168.91.2 - - [24/Apr/2018 11:52:16] \"POST /update HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Master: 192.168.91.1:5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.91.3 - - [24/Apr/2018 11:52:19] \"GET /model HTTP/1.1\" 200 -\n",
      "192.168.91.2 - - [24/Apr/2018 11:52:20] \"GET /model HTTP/1.1\" 200 -\n",
      "192.168.91.3 - - [24/Apr/2018 11:52:59] \"POST /update HTTP/1.1\" 200 -\n",
      "192.168.91.2 - - [24/Apr/2018 11:53:00] \"POST /update HTTP/1.1\" 200 -\n",
      "192.168.91.1 - - [24/Apr/2018 11:53:00] \"POST /shutdown HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit requested...\n",
      "CPU times: user 9.72 s, sys: 3.01 s, total: 12.7 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with DeepDist(doc2VecModel,master='192.168.91.1:5000') as dd:\n",
    "    for i in range(3):\n",
    "        dd.train(corpus,gradient,descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843\n"
     ]
    }
   ],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000)\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS' + \"%09d\"%(i+1,)\n",
    "    prefix_train_neg = 'TRAIN_NEG' + \"%09d\"%(i+1,)\n",
    "    train_arrays[i] = dd.model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] =dd.model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0\n",
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST__POS' + \"%09d\"%(i+1,)\n",
    "    prefix_test_neg = 'TEST__NEG' + \"%09d\"%(i+1,)\n",
    "    test_arrays[i] = dd.model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = dd.model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "\n",
    "print classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'queen',\n",
       " u'prince',\n",
       " u'empress',\n",
       " u'publisher',\n",
       " u'donnelly',\n",
       " u'misa',\n",
       " u'maid',\n",
       " u'diddy',\n",
       " u'gellar',\n",
       " u'westley']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dd.model.most_similar(positive=['king','woman'],negative=['man'])\n",
    "map(lambda x: x[0],a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
